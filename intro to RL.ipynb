{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Welcome to this lecture of Reinforcement Learning. Reinforcement Learning is ultimately about building code capable of learning to perform complex tasks from scratch by itself. It is basically a learning based on a trial and error approach. Many recent applications like robots capable of walking or Alpha Go, Self Driving Cars, are based on this kind of paradigm. \n",
    "\n",
    "e.g. Example https://www.youtube.com/watch?v=gn4nRCC9TwQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "* TD-Gammon, one of the first successful applications of neural networks to reinforcement learning.\n",
    "* AlphaGo Zero, the state-of-the-art computer program that defeats professional human Go players, eg. Google\n",
    "* Learn about how reinforcement learning (RL) is used to play Atari games.\n",
    "* Research used to teach humanoid bodies to walk.\n",
    "* Self-driving cars, eg. Google, Amazon, Uber\n",
    "* Reinforcement Learning for telecommunication, eg. Google\n",
    "* Reinforcement Learning for inventory management, eg. Amazon\n",
    "\n",
    "Could you give any example of Reinforcement Learning you know? \n",
    "\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* **Agent**: The learner or decision maker. You can think of that like a small puppy.\n",
    "\n",
    "* **Environment**: The space problem, however you can visualize it as a trainer to the puppy. The environment is the place where the Agent carries out its tasks. \n",
    "\n",
    "* **State**: The observation of the environment gives the Agent a knowledge of the state of the environment. Example: if it's a robot the states are the context provided for the agent for choosing intelligent actions, that's to say in this case, the positions and velocities of the joints along with some measurements of the ground, and a contact sensor data in order to know if the robot is still walking or not. Based on the information of the state, the Agent needs to choose which action comes next. \n",
    "\n",
    "* **Actions**: The task to perform whatsoever. Example: if it's a robot, the action can be the force that the robot applies to its joints at every point time or timestep. The action has an influence over the environment, thus a response (feedback) is produced from the Environment, as a Reward. \n",
    "\n",
    "* **Reward**: A positive or negative feedback given by the Environment as a consequence of an Action. \n",
    "\n",
    "* **Timestep**: Think of this in terms of time, since it is a learning process. \n",
    "\n",
    "\n",
    "<img src=\"images/agent-world.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "> **Andriy Burkov** in his **The Hundred Page Machine Learning Book** describes reinforcement learning as:\n",
    "\n",
    "> **--Reinforcement learning solves a particular kind of problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics.**\n",
    "\n",
    "<img src=\"images/states-actions-reward_2.gif\" />\n",
    "\n",
    "\n",
    "### Assumptions \n",
    "\n",
    "* The Agent wants to win and have a positive actitud\n",
    "* The Agent observes the environment\n",
    "* Does the Agent have all the information needed to make a good decision?\n",
    "* Is the all the environment fully observable? or only a certain part of the environment is known by the agent? \n",
    "* **THE GOAL OF THE AGENT IS TO MAXIMIZE THE MAXIMIZE THE CUMULATIVE REWARD.** In other words, the Agent must learn to play by the rules of the Environment. Thus, if you think you can solve a problem using RL, you will need to define the states, actions and rewards and also the rules of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Reinforcement Learning\n",
    "\n",
    "There are different types to address a Reinforcement Learning problem, each type corresponds to a different implementation approach and thus, to different algorithms. However they can be grouped in two main categories:  **Model Free and Model Based**: Model Based methods try and make a model of the environment and predict what will happen next in order to make the best decisions. Model Free methods directly connects observations and actions to make optimal decisions. \n",
    "\n",
    "* **Model Based**:  Try to define real-world problems as Markov Decissions Processes (MDP's) or classical methods to solve several environments.  These algorithms aim to learn how the environment works (its dynamics) from its observations and then plan a solution using that model. When they have a model, they use some planning method to find the best policy. They known to be data efficient, but they fail when the state space is too large. Dynamic programming methods are an example of model-based methods, as they require the complete knowledge of the environment, such as transition probabilities and rewards.\n",
    "\n",
    "\n",
    "* **Model Free**: This type of algorightms do not require to learn the environment and store all the combination of states and actions. The can be divided into two categories, based on the ultimate goal of the training.\n",
    "\n",
    "\n",
    "<img src=\"images/rl-algorithms-ex.png\" />\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Policy-based methods try to find the optimal policy, whether it’s **stochastic** or **deterministic**. Algorithms like policy gradients and REINFORCE belong in this category. Their advantages are better convergence and effectiveness on high dimensional or continuous action spaces.\n",
    "\n",
    "> * **Policy-based methods** are essentially an optimization problem, where we find the maximum of a policy function. That’s why we also use algorithms like evolution strategies and hill climbing. Policy Based agents builds a policy over time that determines the probablity of taking a certain action in a given state. \n",
    "\n",
    "\n",
    "> * **Value-based methods**, on the other hand, try to find the optimal value. A big part of this category is a family of algorithms called Q-learning, which learn to optimize the Q-Value. I plan to analyze Q-learning thoroughly on a next article because it is an essential aspect of Reinforcement learning. Other algorithms involve SARSA and value iteration. Value Based agents calculates the value of taking every action and uses that value to choose the best action. Deep Q-Networks (DQN) algorithm uses this approach. \n",
    "\n",
    "At the intersection of policy and value-based method, we find the Actor-Critic methods, where the goal is to optimize both the policy and the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based: RL Framework\n",
    "\n",
    "\n",
    "### Exploration vs Exploitation Dilemma (environment)\n",
    "\n",
    "Let's suppose the puppie (the agent) knows how to behave versus some known situations, instead of taking the risk of try randomly new actions from the actions set possible? \n",
    "\n",
    "* Exploration of potential hypotheses for how to choose actions\n",
    "* Exploiting limited knowledge about what is already known that should work well\n",
    "\n",
    "\n",
    "### Episodic vs Continuing tasks (timestep)\n",
    "\n",
    "Not all the problems have a ending point, eg. wining a game vs the finantial bots that are trading all the time. In the first case, the Agent eventually will be off the game, on the second case, the Agent lives forever and therefore the learning process has no ending. \n",
    "\n",
    "This is an episodic task, where an episode finishes when the game ends. The idea is that by playing the game many times, or by interacting with the environment in many episodes, you can learn to play chess better and better.\n",
    "\n",
    "Let's say you are playing chess, and you only have a reward at the end of the game. It's important to note that this problem is exceptionally difficult, because the feedback is only delivered at the very end of the game. So, if you lose a game (and get a reward of -1 at the end of the episode), it’s unclear when exactly you went wrong: maybe you were so bad at playing that every move was horrible, or maybe instead … you played beautifully for the majority of the game, and then made only a small mistake at the end. When the reward signal is largely uninformative in this way, we say that the task suffers the problem of **sparse rewards**. There’s an entire area of research dedicated to this problem. \n",
    "\n",
    "### The Reward Hypothesis (reward)\n",
    "\n",
    "How do we design a good reward? How do we make a significant enough reward to the problem? Reinforcement is a term originally from behavioural science. It's important to note that the term \"Reinforcement Learning\" comes from behavioral science. It refers to a stimulus that's delivered immediately after behavior to make the behavior more likely to occur in the future. It's an important to defining hypothesis and reinforcement learning that we can always formulate an agents goal,\n",
    "along the lines of maximizing expected cumulative reward. And we call this hypothesis, the \"Reward Hypothesis\".\n",
    "\n",
    "#### Cumulative reward\n",
    "\n",
    "Example https://www.youtube.com/watch?v=gn4nRCC9TwQ [Reward function]\n",
    "\n",
    "The Agent needs to learn about the complex effect of its actions over the environment over time. **This is why we need a cumulative reward, this way the robot is able to make long-term decisions.**\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider an agent who would like to learn to escape a maze. Which reward signals will encourage the agent to escape the maze as quickly as possible? \n",
    "\n",
    " \n",
    "* The reward is -1 for every time step that the agent spends inside the maze. Once the agent escapes, the episode terminates.\n",
    "* The reward is +1 for every time step that the agent spends inside the maze. Once the agent escapes, the episode terminates.\n",
    "* The reward is -1 for every time step that the agent spends inside the maze. Once the agent escapes, it receives a reward of +10, and the episode terminates.\n",
    "* The reward is 0 for every time step that the agent spends inside the maze. Once the agent escapes, it receives a reward of +1, and the episode terminates.\n",
    "\n",
    "<img src=\"images/google_robot_reward_function.png\" width=\"500\"/>\n",
    "\n",
    "#### Return and discounted return\n",
    "\n",
    "The Agent has to choose its Actions to maximize the goals, but who knows what the future holds? That is to say, how is time affecting to the lifelong decisions? Which strategy is better at a short/long term? Maybe the rewards that are closer in time should be more weighted since are more predictible. \n",
    "\n",
    "<img src=\"images/return_at_timestep.png\" width=400 />\n",
    "<img src=\"images/discount.png\" width=400 />\n",
    "<img src=\"images/discounted_return.png\" width=400 />\n",
    "\n",
    "\n",
    "## The formal definition of a problem: Markov Decision Process (MDP) \n",
    "\n",
    "A (finite) Markov Decision Process (MDP) is defined by: \n",
    "\n",
    "* a (finite) set of States $S$\n",
    "* a (finite) set of Actions $A$\n",
    "* a (finite) set of Rewards $R$\n",
    "\n",
    "#### Example vacuum cleaner\n",
    "<img src='images/vacuum_cleaner_actions_states.png' />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a **Vacuum cleaner** which has: \n",
    "\n",
    "* $A = ${$  Search, Recharge, Wait  $}\n",
    "* $S = ${$ High, Low $} \n",
    "\n",
    "Let's suppose the Vacuum cleaner battery is high, so it is then less likely it recharges, so the possible actions are to wait or to search. If it waits, that does not consume battery at all, so at a timestep, the battery will still full. If the robot does that, then it gets a reward of +1. Instead, if it decides to search the space a collect garbage, it is still likely that the battery remains full (70%) but there is also a probability that the battery gets low. Either the case, the reward will be +4. If the battery gets low, the robot can choose among recharge (the most likely option that leads to new state of high), wait and getting a reward of +1 or take the risk of searching the space. If the robot battery completely dies and needs to be rescued, it gets a reward of -3 but if it collects something, gets a reward of +4. \n",
    "\n",
    "\n",
    "### Policies\n",
    "\n",
    "So far, we have a formal definition of our problem, but how might we have a formal defition of the solution? We can start to think of the solution as a series of actions that need to be learned by the agent towards the pursuit of a goal.\n",
    "\n",
    "If you think of Google's humanoid, for instance, in order to walk, the humanoid robot needs to learn the appropriate way to apply forces to its joint. But the correct action changes the situation. If a robot encounters a wall, the best series of actions will be different than if it had nothing blocking its path. Reward is always decided in the context of the state that it\n",
    "was decided in along with the state that follows. With this in mind, as long as the agent learns an appropriate action response to any environment state that it can observe, we have a solution to our problem. **This motivates the idea of a policy ($\\pi$)**\n",
    "\n",
    "**The simplest kind of policy is a mapping from the set of environment states to the set of possible actions.** We call this kind of policy a deterministic policy: Income is the state, outcome is the action to take.\n",
    "\n",
    "**Another type of policy that we we'll examine is a stochastic policy**: The stochastic policy will allow the agent to choose actions randomly. We define a stochastic policy as a mapping that accepts an environment state S and action A and returns the probability that the agent takes action A while in state S.\n",
    "\n",
    "Going back to the previous example:\n",
    "\n",
    "#### Deterministic Policy: Example\n",
    "\n",
    "<img src=\"images/markov_dp_deterministic.png\" width=700 />\n",
    "\n",
    "An example deterministic policy $\\pi$: $\\mathcal{S}\\to\\mathcal{A}$ can be specified as:\n",
    "\n",
    "$\\pi(\\text{low})$ = $\\text{recharge}$ \n",
    "\n",
    "$\\pi(\\text{high})$ = $\\text{search}$\n",
    "\n",
    "In this case: \n",
    "\n",
    "* if the battery level is low, the agent chooses to recharge the battery.\n",
    "* if the battery level is high, the agent chooses to search for cans.\n",
    "\n",
    "Therefore, \n",
    "* If the state is _low_, the agent chooses action _search_.\n",
    "* The agent will always _search_ for cans at every time step (whether the battery level is _low_ or _high_).\n",
    "\n",
    "\n",
    "#### Stochastic Policy: Example\n",
    "\n",
    "<img src=\"images/markov_dp_stochastic.png\" width=700 />\n",
    "\n",
    "An example stochastic policy $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$ can be specified as:\n",
    "\n",
    "$\\pi(\\text{recharge} $|$ \\text{low}) = 0.5$\n",
    "\n",
    "$\\pi(\\text{wait}  $|$ \\text{low}) = 0.4$\n",
    "\n",
    "$\\pi(\\text{search} $|$ \\text{low}) = 0.1$\n",
    "\n",
    "$\\pi(\\text{search} $|$ \\text{high}) = 0.9$\n",
    "\n",
    "$\\pi(\\text{wait} $|$ \\text{high}) = 0.1$\n",
    "\n",
    "In this case,\n",
    "\n",
    "* if the battery level is low, the agent recharges the battery with 50% probability, waits for cans with 40% probability, and searches for cans with 10% probability.\n",
    "* if the battery level is high, the agent searches for cans with 90% probability and waits for cans with 10% probability.\n",
    "\n",
    "Therefore, \n",
    "* If the battery level is _high_, the agent chooses to _search_ for a can with 60% probability, and otherwise _waits_ for a can.\n",
    "* If the battery level is _low_, the agent is most likely to decide to _wait_ for cans.\n",
    "\n",
    "\n",
    "\n",
    "### Finding the best policy : introducing state value functions\n",
    "\n",
    "Another example: \n",
    "\n",
    "<img src=\"images/gridworld.png\" width=\"700\"/>\n",
    "\n",
    "Rules of the environment: \n",
    "\n",
    "* So consider this very very small world and an agent who lives in it.\n",
    "* Say the world is primarily composed of nice patches of grass,\n",
    "* Two out of the nine locations in the world have large mountains.\n",
    "* Then we'll think of this as an episodic task where an episode finishes when the agent reaches the goal as quick as possible.\n",
    "* Say that the agent receives a reward of negative one for most transitions.\n",
    "* If An action leads it to encounter a mountain, it receives some reward of negative three.\n",
    "* And if it reaches the goal state, it gets a reward of five and the episode, ends.\n",
    "* So we can think of the reward signal as punishing the agent for every timestep that it spends away from the goal.\n",
    "* You can think of the mountains as having a special larger punishment because they take even longer to cross than the patches of grass.\n",
    "* The Agent can start at any possible location. \n",
    "\n",
    "**How might we find the best policy here? Let's try the worst policy possible.**\n",
    "\n",
    "<img src=\"images/gridworld-worst-policy.png\" width=\"700\" /> \n",
    "\n",
    "Likewise, we can consider another locations where the Agent might start the game and reach the goal. If we keep track of all this possible rewards, we can get a table like this. \n",
    "\n",
    "<img src=\"images/state-value-function-1.png\" width=\"700\" />\n",
    "\n",
    "**For each state, the state-value function yields the expected return, if the agent started at that state and followed the policy for all time steps.** The state-value function always belongs to a particular policy, thus if we change the policy, we change the state-value function as well. The formal notation is shown below: \n",
    "\n",
    "<img src=\"images/state-value-function-2.png\" width=\"700\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equations\n",
    "\n",
    "The term **dynamic programming (DP)** refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically\n",
    "\n",
    "If you try to calculate all the possible rewards, you might find this process redundant, since the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state.\n",
    "\n",
    "<img src=\"images/bellman-equation-1.png\" width=\"500\"/>\n",
    "<img src=\"images/bellman-equation-2.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "### Optimality \n",
    "\n",
    "<img src=\"images/optimal-policies.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "### Action-value functions\n",
    "\n",
    "State value function for a policy: Meaning for each state s, it yields the expected discounted return If the agent starts in state as and then uses the policy to choose its actions for all time steps. And now, you know how to calculate the state value function corresponding to a policy. **Action-value function**  --is denoted with a lowercase q instead of v--: \n",
    "\n",
    "> While the state values are a function of the environment state, the action values are a function of the environment state and the agent's action. For each state s and action a, the action value function yields the expected discounted return If the agent starts in state s then chooses action a and then uses a policy to choose its actions for all future time steps.\n",
    "\n",
    "\n",
    "<img src=\"images/action-value_vs_state-value.png\" width=\"700\"/>\n",
    "\n",
    "In the case of the state value function, we kept track of the value of each state with the number on the grid. We'll do something similar with the action-value function, where we now need up to four values for each state, each corresponding to a different action.\n",
    "\n",
    "\n",
    "<img src=\"images/action-value-policy.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model free approach: Value-Based Methods\n",
    "\n",
    "### Deep Q-Networks\n",
    "\n",
    "So far, you've solved many of your own reinforcement learning problems, using solution methods that represent the action values in a small table. Earlier in the nanodegree, we referred to this table as a Q-table.\n",
    "\n",
    "Deep Q-Learning algorithm represents the optimal action-value function $q*$ as a neural network (instead of a table).\n",
    "\n",
    "Unfortunately, reinforcement learning is notoriously **unstable when neural networks are used to represent the action values**.  Could you say why not to use a conventional Neural Network? \n",
    "Deep Q-Learning algorithm is aimed to address these instabilities by using two key features:\n",
    "\n",
    "* Experience Replay\n",
    "* Fixed Q-Targets\n",
    "\n",
    "In 2015, Deep Mind made a breakthrough by designing an agent that learned to play video games better than humans. This agent was only given raw pixel data, what a human player would see on screen. And it learned to play a bunch of different Atari games, all from scratch. They call this agent a Deep Q Network. At the heart of the agent is a deep neural network that acts as a function approximator. You pass in images from your favorite video game one screen at a time, and it produces a vector of action values,\n",
    "with the max value indicating the action to take. As a reinforcement signal, it is fed back the change in game score at each time step. In the beginning when the neural network is initialized with random values, the actions taken are all over the place.\n",
    "\n",
    "Consider how complex the input space is: \n",
    "\n",
    "* Atari games are displayed at a resolution of 210 by 160 pixels,\n",
    "* 128 possible colors for each pixel.\n",
    "\n",
    "The Deep Mind team decided to perform some minimal processing, convert the frames to gray scale, and scale them down to a square 84 by 84 pixel block. In order to give the agent access to a sequence of frames, they stacked four such frames together resulting in a final state space size of 84 by 84 by 4.\n",
    "\n",
    "On the output side, unlike a traditional reinforcement learning setup where only one Q value is produced at a time, the Deep Q network is designed to produce a Q value for every possible action in a single forward pass. Instead, you can now simply use this vector to take an action, either stochastically, or by choosing the one with the maximum value. These convolutional layers also extract some temporal properties across those frames. **Therefore, the DQN takes the state as input, and returns the corresponding predicted action values for each possible game action.**\n",
    "\n",
    "<img src=\"images/pong-convolution-DQN.png\" />\n",
    "\n",
    "\n",
    "There are situations where the network weights can oscillate or diverge, due to the high correlation between actions and states.\n",
    "This can result in a very unstable and ineffective policy. In order to overcome these challenges, the researchers came up with several techniques that slightly modified the base Q learning algorithm.\n",
    "\n",
    "<img src=\"images/DeepQN.png\" width=700 />\n",
    "\n",
    "\n",
    "**The Deep Q-Learning algorithm uses two separate networks with identical architectures.**\n",
    "\n",
    "\n",
    "#### Experience replay\n",
    "\n",
    "When the agent interacts with the environment, **the sequence of experience tuples can be highly correlated**. The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. **Why is this so bad**? (Exploration vs Explotation)\n",
    "\n",
    "By instead keeping track of a replay buffer and using experience replay to sample from the buffer at random, we can prevent action values from oscillating or diverging catastrophically.\n",
    "\n",
    "The replay buffer contains a collection of experience tuples $(S, A, R, S')$. The tuples are gradually added to the buffer as we are interacting with the environment.\n",
    "\n",
    "The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.\n",
    "\n",
    "Two important consequences are derivated from this new scenario: \n",
    "\n",
    "* Addressing a RL problem as a Supervised Learning problem. \n",
    "* Prioritize Experience Replay that are rare or more important.\n",
    "\n",
    "#### Fixed Q-Targets\n",
    "\n",
    "The second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.\n",
    "\n",
    "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper earlier this year, where they found that it stabilized the training process.\n",
    "\n",
    "The target Q-Network's weights are updated less often (or more slowly) than the primary Q-Network. Without fixed Q-targets, we would encounter a harmful form of correlation, whereby we shift the parameters of the network based on a constantly moving target (e.g. carrot and the donkey)-. \n",
    "\n",
    "##### Implementation\n",
    "\n",
    "* First, we create two networks (DQNetwork, TargetNetwork)\n",
    "* Create a function that will take our DQNetwork parameters and copy them to our TargetNetwork\n",
    "* During the training, we calculate the TD (Temporal Difference or our Loss) target using our target network. \n",
    "* We update the target network with the DQNetwork every tau step (tau is an hyper-parameter that we define)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model free approach: Policy-Based Methods\n",
    "\n",
    "With value-based methods, the agent uses its experience with the environment to maintain an estimate of the optimal action-value function. The optimal policy is then obtained from the optimal action-value function estimate.\n",
    "\n",
    "<img src=\"images/recap-value-based.png\" />\n",
    "\n",
    "**Policy-based methods directly learn the optimal policy**, without having to maintain a separate value function estimate, meaning, **can we estimate directly the best policy without worrying about the value function at all?** The answer is yes.\n",
    "\n",
    "Let's see this example from Open AI Gym: the cart pole\n",
    "\n",
    "<img src=\"images/cartpole.gif\" width=200 />\n",
    "\n",
    "Eventually we can estimate the best move depending on a some input parameters: \n",
    "\n",
    "<img src=\"images/cartpole-network.png\" />\n",
    "\n",
    "How might be optimize the weights of the network in order to find the best policy? The goal of the agent is to find the value of the policy network weights $\\theta$ that maximizes expected return, which we have denoted by $J$.\n",
    "\n",
    "Similarly to Gradient Descent where we try to find the weights that minimize the loss function, it is introduced **Hill Climbing algorithm**, a way to find the weights that maximize the expected return $J$. What's the difference between the return that the agent collects in a single episode $G$ and the expected return $J$ ? In the hill climbing algorithm, the values of $\\theta$ are evaluated according to how much return $G$ they collected in a single episode. \n",
    "\n",
    "To see that this might be a little bit strange, note that due to randomness in the environment (and the policy, if it is stochastic), it is highly likely that if we collect a second episode with the same values for $\\theta$, we'll likely get a different value for the return $G$. \n",
    "\n",
    "\n",
    "You may be wondering why do we need to find optimal policies directly when value-based methods seem to work so well.\n",
    "There are three arguments we'll consider: \n",
    "\n",
    "* simplicity,\n",
    "* stochastic policies, \n",
    "* and continuous action spaces.\n",
    "\n",
    "If We would directly choose an action based on this probability distribution, this is simpler in the sense that we are directly getting to the problem at hand, but it also avoids having to store a bunch of additional data that may or may not always be useful. For example, large portions of the state space may have the same value. Formulating the policy in this manner allows us to make such generalizations where possible and focus more on the complicated regions of the state space.\n",
    "\n",
    "<img src=\"images/policy-deterministic-stochastic.png\" />\n",
    "\n",
    "One of the main advantages of policy-based methods over value-based methods is that they can learn true stochastic policies.\n",
    "Another situation where a stochastic policy helps is when we have aliased states, that is, two or more states that we perceive to be identical but are actually different. In this sense, the environment might be partially observable. \n",
    "\n",
    "A value-based approach tends to learn a deterministic or near deterministic policy, whereas a policy-based approach in this situation can learn the desired stochastic policy.\n",
    "\n",
    "Our final reason for exploring policy-based methods is that they are well-suited for continuous action spaces. When we use a value-based method even with a function approximator, our output consists of a value for each action. Now, if the action space is discrete and there are a finite number of actions, we can easily pick the action with the maximum value.\n",
    "\n",
    "But if the action space is continuous, then this makes operation turns into an optimization problem itself. It's like trying to find a global maximum of a continuous function which is non-trivial, especially if the function is not convex. A similar issue exists in higher dimensional action spaces, lots of possible actions to evaluate.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended links and bibliography:\n",
    "\n",
    "* https://gym.openai.com/\n",
    "* https://eu.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893\n",
    "* https://sergioskar.github.io/Deep_Q_Learning/\n",
    "* https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/#\n",
    "* https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/\n",
    "* https://medium.com/coinmonks/landing-a-rocket-with-simple-reinforcement-learning-3a0265f8b58c\n",
    "* https://medium.com/coinmonks/landing-a-rocket-with-simple-reinforcement-learning-3a0265f8b58c\n",
    "* http://karpathy.github.io/2016/05/31/rl/\n",
    "* https://github.com/mmuppidi/DQN-Atari-Pong\n",
    "* Multi-Arm Bandit problem : https://www.datacamp.com/community/tutorials/introduction-reinforcement-learning\n",
    "* Reinforcement Learning An Introduction (Sutton and Barto): https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "* https://sergioskar.github.io/Taking_Deep_Q_Networks_a_step_further/\n",
    "* https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/\n",
    "* https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
